{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505fe769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame({\n",
    "    \"n\": np.random.randn(20),\n",
    "    \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "    \"abool\": np.random.choice([True, False], 20),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b3b5d1",
   "metadata": {},
   "source": [
    "1. Spark Dataframe Basics\n",
    "\n",
    "    1. Use the starter code above to create a pandas dataframe.\n",
    "    1. Convert the pandas dataframe to a spark dataframe. From this point\n",
    "       forward, do all of your work with the spark dataframe, not the pandas\n",
    "       dataframe.\n",
    "    1. Show the first 3 rows of the dataframe.\n",
    "    1. Show the first 7 rows of the dataframe.\n",
    "    1. View a summary of the data using `.describe`.\n",
    "    1. Use `.select` to create a new dataframe with just the `n` and `abool`\n",
    "       columns. View the first 5 rows of this dataframe.\n",
    "    1. Use `.select` to create a new dataframe with just the `group` and `abool`\n",
    "       columns. View the first 5 rows of this dataframe.\n",
    "    1. Use `.select` to create a new dataframe with the `group` column and the\n",
    "       `abool` column renamed to `a_boolean_value`. Show the first 3 rows of\n",
    "       this dataframe.\n",
    "    1. Use `.select` to create a new dataframe with the `group` column and the\n",
    "       `n` column renamed to `a_numeric_value`. Show the first 6 rows of this\n",
    "       dataframe.\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb82a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/homebrew/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/05 10:08:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(n=-0.712390662050588, group='z', abool=False),\n",
       " Row(n=0.753766378659703, group='x', abool=False)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to spark dataframe\n",
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1bb3778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, n: string, group: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first three rows, 7 rows, describe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a355275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(number=-0.712390662050588, a_boolean_value=False),\n",
       " Row(number=0.753766378659703, a_boolean_value=False),\n",
       " Row(number=-0.044503078338053455, a_boolean_value=False),\n",
       " Row(number=0.45181233874578974, a_boolean_value=False),\n",
       " Row(number=1.3451017084510097, a_boolean_value=False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .select operations\n",
    "n_abool_df = df.select(df.n.alias('number'),df.abool.alias('a_boolean_value'))\n",
    "n_abool_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78450324",
   "metadata": {},
   "source": [
    "2. Column Manipulation\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe. Store the\n",
    "       spark dataframe in a varaible named `df`\n",
    "\n",
    "    1. Use `.select` to add 4 to the `n` column. Show the results.\n",
    "\n",
    "    1. Subtract 5 from the `n` column and view the results.\n",
    "\n",
    "    1. Multiply the `n` column by 2. View the results along with the original\n",
    "       numbers.\n",
    "\n",
    "    1. Add a new column named `n2` that is the `n` value multiplied by -1. Show\n",
    "       the first 4 rows of your dataframe. You should see the original `n` value\n",
    "       as well as `n2`.\n",
    "\n",
    "    1. Add a new column named `n3` that is the n value squared. Show the first 5\n",
    "       rows of your dataframe. You should see both `n`, `n2`, and `n3`.\n",
    "\n",
    "    1. What happens when you run the code below?\n",
    "\n",
    "        ```python\n",
    "        df.group + df.abool\n",
    "        ```\n",
    "\n",
    "    1. What happens when you run the code below? What is the difference between\n",
    "       this and the previous code sample?\n",
    "\n",
    "        ```python\n",
    "        df.select(df.group + df.abool)\n",
    "        ```\n",
    "\n",
    "    1. Try adding various other columns together. What are the results of\n",
    "       combining the different data types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb73e45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(n=-0.712390662050588, group='z', abool=False, add_4=3.2876093379494122),\n",
       " Row(n=0.753766378659703, group='x', abool=False, add_4=4.753766378659703)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add, subtract, multiply\n",
    "df.select('*', (df.n + 4).alias('add_4')).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a42dcb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(group + abool)'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.group + df.abool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066f4b8",
   "metadata": {},
   "source": [
    "3. Type casting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "    1. Use `.printSchema` to view the datatypes in your dataframe.\n",
    "\n",
    "    1. Use `.dtypes` to view the datatypes in your dataframe.\n",
    "\n",
    "    1. What is the difference between the two code samples below?\n",
    "\n",
    "        ```python\n",
    "        df.abool.cast('int')\n",
    "        ```\n",
    "\n",
    "        ```python\n",
    "        df.select(df.abool.cast('int')).show()\n",
    "        ```\n",
    "\n",
    "    1. Use `.select` and `.cast` to convert the `abool` column to an integer\n",
    "       type. View the results.\n",
    "    1. Convert the `group` column to a integer data type and view the results.\n",
    "       What happens?\n",
    "    1. Convert the `n` column to a integer data type and view the results. What\n",
    "       happens?\n",
    "    1. Convert the `abool` column to a string data type and view the results.\n",
    "       What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "162ba455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(abool='false'), Row(abool='false')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df.abool.cast('string')).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8214ec",
   "metadata": {},
   "source": [
    "4. Built-in Functions\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Import the necessary functions from `pyspark.sql.functions`\n",
    "    1. Find the highest `n` value.\n",
    "    1. Find the lowest `n` value.\n",
    "    1. Find the average `n` value.\n",
    "    1. Use `concat` to change the `group` column to say, e.g. \"Group: x\" or\n",
    "       \"Group: y\"\n",
    "    1. Use `concat` to combine the `n` and `group` columns to produce results\n",
    "       that look like this: \"x: -1.432\" or \"z: 2.352\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16b8aaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            max(n)|\n",
      "+------------------+\n",
      "|2.1503829673811126|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min, mean, concat, lit\n",
    "df.select(max(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd8d33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(group_vals='z: -0.712390662050588'),\n",
       " Row(group_vals='x: 0.753766378659703'),\n",
       " Row(group_vals='z: -0.044503078338053455')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(concat(lit(df.group), lit(': '), df.n).alias('group_vals')).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee308649",
   "metadata": {},
   "source": [
    "5. When / Otherwise\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Use `when` and `.otherwise` to create a column that contains the text \"It\n",
    "       is true\" when `abool` is true and \"It is false\"\" when `abool` is false.\n",
    "    1. Create a column that contains 0 if n is less than 0, otherwise, the\n",
    "       original n value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c63982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+\n",
      "|abool|    truths|    falsths|\n",
      "+-----+----------+-----------+\n",
      "|false|      null|It is false|\n",
      "|false|      null|It is false|\n",
      "|false|      null|It is false|\n",
      "|false|      null|It is false|\n",
      "|false|      null|It is false|\n",
      "|false|      null|It is false|\n",
      "|false|      null|It is false|\n",
      "|false|      null|It is false|\n",
      "| true|It is true|       null|\n",
      "| true|It is true|       null|\n",
      "|false|      null|It is false|\n",
      "|false|      null|It is false|\n",
      "| true|It is true|       null|\n",
      "| true|It is true|       null|\n",
      "|false|      null|It is false|\n",
      "|false|      null|It is false|\n",
      "|false|      null|It is false|\n",
      "| true|It is true|       null|\n",
      "|false|      null|It is false|\n",
      "| true|It is true|       null|\n",
      "+-----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# B\n",
    "from pyspark.sql.functions import when\n",
    "df.select('abool', \n",
    "          when(df.abool, 'It is true').alias('truths'), \n",
    "          when(~ df.abool, 'It is false').alias('falsths')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd202369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(0 or greater=0.0),\n",
       " Row(0 or greater=0.753766378659703),\n",
       " Row(0 or greater=0.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C\n",
    "df.select(when(df.n < 0, 0).otherwise(df.n).alias('0 or greater')).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ede688",
   "metadata": {},
   "source": [
    "6. Filter / Where\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Use `.filter` or `.where` to select just the rows where the group is `y`\n",
    "       and view the results.\n",
    "    1. Select just the columns where the `abool` column is false and view the\n",
    "       results.\n",
    "    1. Find the columns where the `group` column is *not* `y`.\n",
    "    1. Find the columns where `n` is positive.\n",
    "    1. Find the columns where `abool` is true and the `group` column is `z`.\n",
    "    1. Find the columns where `abool` is true or the `group` column is `z`.\n",
    "    1. Find the columns where `abool` is false and `n` is less than 1\n",
    "    1. Find the columns where `abool` is false or `n` is less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f89d2a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(n=1.4786857374358966, group='z', abool=True),\n",
       " Row(n=-0.026771649986440726, group='x', abool=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where((df.group != 'y') & (df.abool)).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a47b31",
   "metadata": {},
   "source": [
    "7. Sorting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Sort by the `n` value.\n",
    "    1. Sort by the `group` value, both ascending and descending.\n",
    "    1. Sort by the group value first, then, within each group, sort by `n`\n",
    "       value.\n",
    "    1. Sort by `abool`, `group`, and `n`. Does it matter in what order you\n",
    "       specify the columns when sorting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680e562f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(n=-0.026771649986440726, group='x', abool=True),\n",
       " Row(n=0.753766378659703, group='x', abool=False),\n",
       " Row(n=-0.7889890249515489, group='x', abool=False),\n",
       " Row(n=0.6062886568962988, group='x', abool=False)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort(df.group.asc(), df.abool.desc()).head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8880de",
   "metadata": {},
   "source": [
    "8. Spark SQL\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Turn your dataframe into a table that can be queried with spark SQL. Name\n",
    "       the table `my_df`. Answer the rest of the questions in this section with\n",
    "       a spark sql query (`spark.sql`) against `my_df`. After each step, view\n",
    "       the first 7 records from the dataframe.\n",
    "    1. Write a query that shows all of the columns from your dataframe.\n",
    "    1. Write a query that shows just the `n` and `abool` columns from the\n",
    "       dataframe.\n",
    "    1. Write a query that shows just the `n` and `group` columns. Rename the\n",
    "       `group` column to `g`.\n",
    "    1. Write a query that selects `n`, and creates two new columns: `n2`, the\n",
    "       original `n` values halved, and `n3`: the original n values minus 1.\n",
    "    1. What happens if you make a SQL syntax error in your query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc328be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(n=-0.712390662050588, group='z', abool=False),\n",
       " Row(n=0.753766378659703, group='x', abool=False),\n",
       " Row(n=-0.044503078338053455, group='z', abool=False)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inital creation\n",
    "df.createOrReplaceTempView('df')\n",
    "spark.sql(''' SELECT * FROM df ''').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7414cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                   n|                  n2|                  n3|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|  -0.712390662050588|  -0.356195331025294|  -1.712390662050588|\n",
      "|   0.753766378659703|  0.3768831893298515|-0.24623362134029703|\n",
      "|-0.04450307833805...|-0.02225153916902...| -1.0445030783380536|\n",
      "| 0.45181233874578974| 0.22590616937289487| -0.5481876612542103|\n",
      "|  1.3451017084510097|  0.6725508542255049| 0.34510170845100974|\n",
      "|  0.5323378882945463| 0.26616894414727316| -0.4676621117054537|\n",
      "|  1.3501878997225267|  0.6750939498612634| 0.35018789972252673|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL queries\n",
    "spark.sql(\"\"\" select n, (n / 2) as n2, (n - 1) as n3 from df \"\"\").show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e710cd",
   "metadata": {},
   "source": [
    "9. Aggregating\n",
    "\n",
    "    1. What is the average `n` value for each group in the `group` column?\n",
    "    1. What is the maximum `n` value for each group in the `group` column?\n",
    "    1. What is the minimum `n` value by `abool`?\n",
    "    1. What is the average `n` value for each unique combination of the `group`\n",
    "       and `abool` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd4b79a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|group|            avg(n)|\n",
      "+-----+------------------+\n",
      "|    x|0.2871427762539448|\n",
      "|    z| 0.590730814237962|\n",
      "|    y| 0.257601419602374|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy('group').agg(mean(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6225e9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------+\n",
      "|group|abool|              avg(n)|\n",
      "+-----+-----+--------------------+\n",
      "|    z|false| 0.41313982959837514|\n",
      "|    x|false|  0.3499256615020219|\n",
      "|    y|false| 0.15907124664523611|\n",
      "|    y| true| 0.35613159255951177|\n",
      "|    z| true|  1.4786857374358966|\n",
      "|    x| true|-0.02677164998644...|\n",
      "+-----+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('group','abool').agg(mean(df.n)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91936670",
   "metadata": {},
   "source": [
    "Revisit the exercises for the [pandas dataframes lesson][1] and [the advanced\n",
    "dataframes lesson][2]. Complete the exercises, but convert the pandas dataframes\n",
    "to spark dataframes first in order to practice using the spark api.\n",
    "\n",
    "[1]: https://ds.codeup.com/python/dataframes/\n",
    "[2]: https://ds.codeup.com/python/advanced-dataframes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7458c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
